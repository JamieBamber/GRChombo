#!/bin/bash
# Job Name and Files (also --job-name)
#SBATCH -J KerrSFjob

#Output and error (also --output, --error):
#SBATCH -o ./%x.%j.out
#SBATCH -e ./%x.%j.err

#Initial working directory (also --chdir):
#SBATCH -D ./

#Notification and type
#SBATCH --mail-type=ALL
#SBATCH --mail-user=james.bamber@physics.ox.ac.uk

# Wall clock limit:
#SBATCH --time=00:10:00
#SBATCH --no-requeue

#Setup of execution environment
#SBATCH --export=NONE
#SBATCH --get-user-env
#SBATCH --account=pn34tu
#SBATCH --partition=test

#Number of nodes and MPI tasks per node:
#SBATCH --nodes=2
# There are 48 physical cores per node, so
# the product of these numbers should be 48
#SBATCH --ntasks-per-node=12
export OMP_NUM_THREADS=4

#Load modules
module load slurm_setup
module load intel mkl hdf5 gcc/7
 
#Default Pinning:
#Thread0/Task0 ->CPU0 or CPU48
#Thread1/Task0 ->CPU1 or CPU49
#Thread2/Task0 ->CPU2 or CPU50

#! Full path to application executable: 
application="/dss/dsshome1/04/di76bej/GRChombo/GRChombo/Examples/BinaryBHScalarField_test/Main_BinaryBH3d.Linux.64.mpiicpc.ifort.OPT.MPI.ex"

#! Run options for the application:
currentdir=$(pwd)
options="${currentdir}/../params.txt"

#Run the program:
mpiexec -n $SLURM_NTASKS $application $options

