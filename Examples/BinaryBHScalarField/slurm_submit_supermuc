#!/bin/bash
# Job Name and Files (also --job-name)
<<<<<<< HEAD
#SBATCH -J BinaryBHSF_Job
=======
#SBATCH -J JOBNAME
>>>>>>> 19ec65fc4645d129b84ac21c2a7df23148a31ffb

#Output and error (also --output, --error):
#SBATCH -o ./%x.%j.out
#SBATCH -e ./%x.%j.err

#Initial working directory (also --chdir):
#SBATCH -D ./

#Notification and type
#SBATCH --mail-type=ALL
#SBATCH --mail-user=james.bamber@physics.ox.ac.uk

# Wall clock limit:
<<<<<<< HEAD
#SBATCH --time=12:00:00
=======
#SBATCH --time=36:00:00
>>>>>>> 19ec65fc4645d129b84ac21c2a7df23148a31ffb
#SBATCH --no-requeue

#Setup of execution environment
#SBATCH --export=NONE
#SBATCH --get-user-env
#SBATCH --account=pn34tu
#SBATCH --partition=general

#Number of nodes and MPI tasks per node:
<<<<<<< HEAD
#SBATCH --nodes=25
# There are 48 physical cores per node, so
# the product of these numbers should be 48
#SBATCH --ntasks-per-node=8
export OMP_NUM_THREADS=4
=======
#SBATCH --nodes=43
# There are 48 physical cores per node, so
# the product of these numbers should be 48
#SBATCH --ntasks-per-node=6
export OMP_NUM_THREADS=8
>>>>>>> 19ec65fc4645d129b84ac21c2a7df23148a31ffb

#Load modules
module load slurm_setup
module load intel mkl hdf5 gcc/7
 
#Default Pinning:
#Thread0/Task0 ->CPU0 or CPU48
#Thread1/Task0 ->CPU1 or CPU49
#Thread2/Task0 ->CPU2 or CPU50

#! Full path to application executable: 
application="/dss/dsshome1/04/di76bej/GRChombo/GRChombo/Examples/BinaryBHScalarField/Main_BinaryBH3d.Linux.64.mpiicpc.ifort.OPTHIGH.MPI.ex"

#! Run options for the application:
currentdir=$(pwd)
options="${currentdir}/../params.txt"

#Run the program:
mpiexec -n $SLURM_NTASKS $application $options

